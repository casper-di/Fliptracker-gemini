# üö® Solution NLP pour Render.com

## ‚ùå Probl√®me

**Ollama ne peut PAS √™tre d√©ploy√© sur Render** car :
- Render n'offre pas d'instances GPU
- Ollama n√©cessite ~4.7GB pour le mod√®le + 2GB RAM en ex√©cution
- Render ne permet pas l'installation de binaires syst√®me (Ollama CLI)
- Les Web Services Render sont √©ph√©m√®res (pas de persistence)

## ‚úÖ Solution Imm√©diate : D√©sactiver NLP en Production

### √âtape 1 : Ajouter la variable d'environnement sur Render

Dans le dashboard Render (Web Service Backend) :

```bash
NLP_ENABLED=false
```

### √âtape 2 : Le syst√®me fonctionnera avec les parsers d√©terministes

Les parsers actuels supportent d√©j√† :
- ‚úÖ Vinted Go (SALE + PURCHASE)
- ‚úÖ Mondial Relay
- ‚úÖ Chronopost
- ‚úÖ Colissimo
- ‚úÖ DHL
- ‚úÖ UPS
- ‚úÖ FedEx

### √âtape 3 : NLP reste disponible en d√©veloppement local

Pour le d√©veloppement local (avec Ollama install√©) :
```bash
# .env local
NLP_ENABLED=true
OLLAMA_HOST=http://localhost:11434
```

---

## üîÄ Alternatives pour NLP en Production

### Option A : API OpenAI (Simple, Rapide)

**Avantages** :
- ‚úÖ D√©ploiement imm√©diat sur Render
- ‚úÖ Scalable automatiquement
- ‚úÖ Pas de serveur √† g√©rer

**Inconv√©nients** :
- ‚ùå Co√ªts r√©currents (~0.002$ / email)
- ‚ùå Donn√©es envoy√©es √† OpenAI (RGPD √† consid√©rer)

**Configuration** :
```bash
# Render Environment Variables
NLP_ENABLED=true
NLP_PROVIDER=openai
OPENAI_API_KEY=sk-xxxxx
```

**Code** (√† modifier dans `nlp.service.ts`) :
```typescript
// Remplacer Ollama par OpenAI SDK
import OpenAI from 'openai';

async refinWithLLM(data: any): Promise<any> {
  if (process.env.NLP_PROVIDER === 'openai') {
    const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini', // Moins cher que GPT-4
      messages: [{ role: 'user', content: this.buildLLMPrompt(data) }],
    });
    return JSON.parse(response.choices[0].message.content);
  }
  // Fallback to Ollama for local dev
  return this.ollama.chat(...);
}
```

**Co√ªts estim√©s** :
- 1000 emails/mois : ~2‚Ç¨
- 10000 emails/mois : ~20‚Ç¨

---

### Option B : Serveur VPS s√©par√© pour Ollama

**Architecture** :
```
Render Backend ‚Üí HTTPS ‚Üí VPS (Ollama) ‚Üí R√©ponse
```

**Configuration VPS** (Hetzner CPX21 - 10‚Ç¨/mois) :
```bash
# Sur le VPS
curl -fsSL https://ollama.com/install.sh | sh
ollama serve --host 0.0.0.0:11434
ollama pull llama3.1:8b-instruct

# Configurer firewall
ufw allow 11434/tcp
ufw enable
```

**Configuration Render** :
```bash
# Render Environment Variables
NLP_ENABLED=true
OLLAMA_HOST=https://votre-vps.com:11434
```

**Co√ªts** : ~10‚Ç¨/mois VPS

---

### Option C : Hugging Face Inference API

**Avantages** :
- ‚úÖ Mod√®les open-source (Llama, Mistral)
- ‚úÖ Pay-per-use (pas de serveur fixe)
- ‚úÖ D√©ploiement imm√©diat

**Inconv√©nients** :
- ‚ö†Ô∏è Cold start (~5-10 secondes premi√®re requ√™te)
- ‚ö†Ô∏è Latence r√©seau

**Configuration** :
```bash
NLP_ENABLED=true
NLP_PROVIDER=huggingface
HF_API_TOKEN=hf_xxxxx
HF_MODEL=meta-llama/Llama-3.1-8B-Instruct
```

**Co√ªts** : ~0.001$ / requ√™te (moins cher qu'OpenAI)

---

## üìä Comparaison

| Solution | Co√ªt/mois | Latence | Setup | RGPD |
|----------|-----------|---------|-------|------|
| **D√©sactiv√©** | 0‚Ç¨ | Instantan√© | ‚úÖ Aucun | ‚úÖ 100% |
| **OpenAI** | 2-20‚Ç¨ | 200ms | ‚úÖ Simple | ‚ö†Ô∏è USA |
| **VPS Ollama** | 10‚Ç¨ | 300ms | ‚ö†Ô∏è Moyen | ‚úÖ EU |
| **HuggingFace** | 1-10‚Ç¨ | 500ms | ‚úÖ Simple | ‚úÖ EU |

---

## üéØ Recommandation

**Pour d√©ployer MAINTENANT** :
1. D√©sactiver NLP (`NLP_ENABLED=false`)
2. Les 7 parsers d√©terministes sont suffisants pour Vinted/Colissimo/etc.
3. Ajouter OpenAI plus tard si besoin (2 lignes de code)

**Pour une solution RGPD compl√®te** :
- D√©ployer un VPS avec Ollama (guide complet disponible)
- Budget : 10‚Ç¨/mois

**Pour tester rapidement l'IA** :
- Utiliser OpenAI GPT-4o-mini (0.002$/email)
- √âvaluer les co√ªts r√©els avant de d√©cider
