name: Train NER Model

on:
  workflow_dispatch: {}
  push:
    branches: [ main ]
    paths: [ 'fliptracker/apps/nlp-service/aa.jsonl.json' ]

jobs:
  train:
    name: Train NER Model
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: fliptracker/apps/nlp-service
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install spacy

      - name: Deep Data Cleaning & Alignment
        run: |
          python -c "
          import spacy, json, os, re
          from spacy.tokens import DocBin
          from spacy.util import filter_spans
          
          nlp = spacy.blank('fr')
          db = DocBin()
          input_file = 'aa.jsonl.json'
          
          if not os.path.exists(input_file):
              print('File not found'); exit(1)

          with open(input_file, 'r', encoding='utf-8') as f:
              content = f.read().strip()
              try:
                  data = json.loads(content)
              except:
                  data = [json.loads(l) for l in content.split('\n') if l.strip()]

          print(f'Analyse de {len(data)} documents...')
          valid_docs = 0

          for item in data:
              try:
                  raw_text = item.get('text') or (json.loads(item['line']).get('text', '') if 'line' in item else '')
                  # Suppression des caractères parasites unicode
                  text = re.sub(r'[\u200b\u200c\u200d\uFEFF]', '', raw_text)
                  if not text.strip(): continue
                  
                  doc = nlp.make_doc(text)
                  ents = []
                  for e in item.get('entities', []):
                      start, end, label = e['start'], e['end'], e['entity']
                      
                      # 1. Extraction et nettoyage des bords du texte annoté
                      span_text = text[start:end]
                      m = re.search(r'[a-zA-Z0-9]', span_text)
                      if not m: continue
                      
                      s_adj = start + m.start()
                      e_adj = start + list(re.finditer(r'[a-zA-Z0-9]', span_text))[-1].end()

                      # 2. ALIGNEMENT 'EXPAND' : On force spaCy à englober les mots entiers
                      # Cela résout l'erreur E024 en s'assurant qu'on ne coupe pas un token.
                      span = doc.char_span(s_adj, e_adj, label=label, alignment_mode='expand')
                      
                      if span:
                          ents.append(span)
                  
                  # 3. Suppression des chevauchements (priorité aux spans les plus longs)
                  filtered = filter_spans(ents)
                  
                  # 4. Vérification finale avant ajout pour éviter les crashs pendant le training
                  try:
                      doc.ents = filtered
                      db.add(doc)
                      valid_docs += 1
                  except:
                      # Si spaCy rejette encore le doc malgré tout, on passe au suivant
                      continue
              except: continue

          db.to_disk('train.spacy')
          print(f'Succès : {valid_docs} documents convertis proprement.')
          "

      - name: Train Model
        run: |
          # Initialisation de la config
          python -m spacy init config config.cfg --lang fr --pipeline ner --optimize efficiency --force
          
          # Entraînement avec gestion des erreurs
          # On utilise le même set pour dev car le volume est encore faible
          python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy

      - name: Upload Model Artifact
        uses: actions/upload-artifact@v4
        with:
          name: fliptracker-model-best
          path: fliptracker/apps/nlp-service/output/model-best